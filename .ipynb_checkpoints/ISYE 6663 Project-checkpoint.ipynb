{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import time\n",
    "from optimizer import Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extended Rosenblack Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set $n = 200$ to test rate of convergence and running time and use different $n$ to compare memory usage between BFGS and L-BFGS methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x0 = np.array([-1.2, 1] * 100) # 200 dimension\n",
    "optimizer_erf = Optimizer('erf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steepest Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- steepest descent start ----\n",
      "minimum: 0.00241920182986\n",
      "iteration: 1537\n",
      "time: 1.01918983459s\n",
      "---- steepest descent end ----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"---- steepest descent start ----\"\n",
    "start = time.time()\n",
    "result_sd = optimizer_erf.steepest_descent(x0)\n",
    "end = time.time()\n",
    "print \"minimum: {}\\niteration: {}\\ntime: {}s\".format(\n",
    "    result_sd[1], result_sd[2], end - start)\n",
    "print \"---- steepest descent end ----\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjugate Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During optimization process of Fletcher-Reeves conjugate gradient method, the line search algorithm provided by scipy.optimize.line_seach() does not converge, causing my program to break down. I implemented a simpler version of line search algorithm which satisfies wolfe condition, slower and unstable, though. This is why my Fletcher-Reeves variant is significantly slower than Polak-Ribiere variant, besides the fact that the latter one required fewer iterations to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- conjugate gradient-fr start ----\n",
      "minimum: 0.00241913238635\n",
      "iteration: 118\n",
      "time: 0.699166059494s\n",
      "---- conjugate gradient-fr end ----\n",
      "\n",
      "---- conjugate gradient-pr start ----\n",
      "iteration 0: value: 2420.0\n",
      "minimum: 0.000250934132895\n",
      "iteration: 16\n",
      "time: 0.0115859508514s\n",
      "---- conjugate gradient-pr end ----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# conjugate gradient - Fletcher-Reeves\n",
    "print \"---- conjugate gradient-fr start ----\"\n",
    "start = time.time()\n",
    "result_cg_fr = optimizer_erf.conjugate_gradient_fr(x0, scipy_ls=False)\n",
    "end = time.time()\n",
    "print \"minimum: {}\\niteration: {}\\ntime: {}s\".format(\n",
    "    result_cg_fr[1], result_cg_fr[2], end - start)\n",
    "print \"---- conjugate gradient-fr end ----\\n\"\n",
    "\n",
    "# conjugate gradient - Polak-Ribiere\n",
    "print \"---- conjugate gradient-pr start ----\"\n",
    "start = time.time()\n",
    "result_cg_pr = optimizer_erf.conjugate_gradient_pr(x0)\n",
    "end = time.time()\n",
    "print \"minimum: {}\\niteration: {}\\ntime: {}s\".format(\n",
    "    result_cg_pr[1], result_cg_pr[2], end - start)\n",
    "print \"---- conjugate gradient-pr end ----\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quasi-Newton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- quasi_newton_bfgs start ----\n",
      "minimum: 0.002353219121\n",
      "iteration: 415\n",
      "time: 0.450258016586s\n",
      "---- quasi_newton_bfgs end ----\n",
      "\n",
      "---- quasi_newton_dfp start ----\n",
      "minimum: 0.00241974037583\n",
      "iteration: 13527\n",
      "time: 9.6368970871s\n",
      "---- quasi_newton_dfp end ----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BFGS\n",
    "print \"---- quasi_newton_bfgs start ----\"\n",
    "start = time.time()\n",
    "result_qn_bfgs = optimizer_erf.quasi_newton_bfgs(x0)\n",
    "end = time.time()\n",
    "print \"minimum: {}\\niteration: {}\\ntime: {}s\".format(\n",
    "    result_qn_bfgs[1], result_qn_bfgs[2], end - start)\n",
    "print \"---- quasi_newton_bfgs end ----\\n\"\n",
    "\n",
    "# DFP\n",
    "print \"---- quasi_newton_dfp start ----\"\n",
    "start = time.time()\n",
    "result_qn_dfp = optimizer_erf.quasi_newton_dfp(x0)\n",
    "end = time.time()\n",
    "print \"minimum: {}\\niteration: {}\\ntime: {}s\".format(\n",
    "    result_qn_dfp[1], result_qn_dfp[2], end - start)\n",
    "print \"---- quasi_newton_dfp end ----\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found that the performance of quasi-newton algorithms was incredibly bad for this function, particularlt DFP algorithm. To check whether it was due to bad implementation, I compared my implementation with the verison Scipy provides. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- My BFGS start ----\n",
      "minimum: 2.37356548384e-10\n",
      "iteration: 591\n",
      "time: 0.580150842667s\n",
      "---- My BFGS end ----\n",
      "\n",
      "---- Scipy BFGS start ----\n",
      "minimum: 1.04187962383e-09\n",
      "iteration: 760\n",
      "time: 1.61571884155s\n",
      "---- Scipy BFGS end ----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# My BFGS\n",
    "print \"---- My BFGS start ----\"\n",
    "start = time.time()\n",
    "result_qn_bfgs = optimizer_erf.quasi_newton_bfgs(x0, eps=1e-13)\n",
    "end = time.time()\n",
    "print \"minimum: {}\\niteration: {}\\ntime: {}s\".format(\n",
    "    result_qn_bfgs[1], result_qn_bfgs[2], end - start)\n",
    "print \"---- My BFGS end ----\\n\"\n",
    "\n",
    "# Scipy BFGS\n",
    "from scipy.optimize import minimize\n",
    "print \"---- Scipy BFGS start ----\"\n",
    "start = time.time()\n",
    "result = minimize(optimizer_erf.f, x0, method='BFGS',\n",
    "                  jac=optimizer_erf.grad_f)\n",
    "end = time.time()\n",
    "print \"minimum: {}\\niteration: {}\\ntime: {}s\".format(\n",
    "    result.fun, result.nit, end - start)\n",
    "print \"---- Scipy BFGS end ----\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turned out that my version achived **better** accuracy with **fewer** iterations in **shorter** time.\n",
    "\n",
    "Therefore, a more reasonable explanation is that the performance of BFGS is worse than that of conjugate gradient methods for this function and the performance of DFP is even worser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limited Memory BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Limited Memory BFGS start ----\n",
      "minimum: 0.00186858918025\n",
      "iteration: 60\n",
      "time: 0.0499820709229s\n",
      "---- Limited Memory BFGS end ----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"---- Limited Memory BFGS start ----\"\n",
    "start = time.time()\n",
    "result_qn_l_bfgs = optimizer_erf.quasi_newton_l_bfgs(x0)\n",
    "end = time.time()\n",
    "print \"minimum: {}\\niteration: {}\\ntime: {}s\".format(\n",
    "   result_qn_l_bfgs[1], result_qn_l_bfgs[2], end - start)\n",
    "print \"---- Limited Memory BFGS end ----\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of limited memory BFGS is significanly better than that of BFGS and DFP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary ($n=200$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Method               | Iteration     | Time(s)      | \n",
    "| -------------        |:-------------:| -----:       |\n",
    "| Steepest Descent     | 1537          | 1.0191898345 |\n",
    "| C-G-FR               | 118           | 0.6991660594 |\n",
    "| C-G-PR               | 16            | 0.0115859508 |\n",
    "| BFGS                 | 415           | 0.4502580165 |\n",
    "| DFP                  | 13527         | 9.6368970871 |\n",
    "| L-BFGS               | 60            | 0.0499820709 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To profile the memory usage of BFGS and L-BFGS methods, I modified my code because the first several iterations of L-BFGS algorithm is the same as that of BFGS algorithm. The diffenrence of memory usage would be negligible without modification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Dimension         | Memory(BFGS)  | Memory(L-BFGS)| \n",
    "| -------------     |:-------------:| -----:        |\n",
    "| 200               | 2.9 Mb        | $<$ 0.1Mb     |\n",
    "| 400               | 6.7 Mb        | $<$ 0.1Mb     |\n",
    "| 800               | 25.3 Mb       | $<$ 0.1Mb     |\n",
    "| 1600              | 99.2 Mb       | $<$ 0.1Mb     |\n",
    "| 3200              | 2.3 Mb        | $<$ 0.1Mb     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used Python memory_profiler module to profile the memory usage. Since L-BFGS depends on BFGS, hence these two algorithms need to run at the same time, so I highly doubt that the memory usage of L-BFGS is underestimated. However, we observe that the memory usage of BFGS grows quadraticly in dimension $n$ when $n$ is large, which is consistent with theoretic analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extended Powell Singular Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set $n=1600$ to test rate of convergence and running time and use different  nn  to compare memory usage between BFGS and L-BFGS methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y0 = np.array([3.0, -1.0, 0.0, 1.0] * 400) # 1600 dimension\n",
    "optimizer_epsf = Optimizer('epsf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steepest Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- steepest descent start ----\n",
      "\n",
      "minimum: 0.0859546513591\n",
      "iteration: 2175\n",
      "time: 3.0121819973s\n",
      "---- steepest descent end ----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# steepest_descent\n",
    "print \"---- steepest descent start ----\"\n",
    "start = time.time()\n",
    "result_sd = optimizer_epsf.steepest_descent(y0)\n",
    "end = time.time()\n",
    "print \"\\nminimum: {}\\niteration: {}\\ntime: {}s\".format(\n",
    "    result_sd[1], result_sd[2], end - start)\n",
    "print \"---- steepest descent end ----\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjugate Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It in interesting that the scipy.optimize.line_search() works well with this function. I did not use my own version of line search algorithm for extended Powell singular function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- conjugate gradient-fr start ----\n",
      "minimum: 0.0859644484262\n",
      "iteration: 71\n",
      "time: 0.110360145569s\n",
      "---- conjugate gradient-fr end ----\n",
      "\n",
      "---- conjugate gradient-pr start ----\n",
      "iteration 0: value: 86000.0\n",
      "minimum: 0.0175363601136\n",
      "iteration: 20\n",
      "time: 0.0370700359344s\n",
      "---- conjugate gradient-pr end ----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# conjugate_gradient_fr\n",
    "print \"---- conjugate gradient-fr start ----\"\n",
    "start = time.time()\n",
    "result_cg_fr = optimizer_epsf.conjugate_gradient_fr(y0)\n",
    "end = time.time()\n",
    "print \"minimum: {}\\niteration: {}\\ntime: {}s\".format(\n",
    "   result_cg_fr[1], result_cg_fr[2], end - start)\n",
    "print \"---- conjugate gradient-fr end ----\\n\"\n",
    "\n",
    "# conjugate_gradient_pr\n",
    "print \"---- conjugate gradient-pr start ----\"\n",
    "start = time.time()\n",
    "result_cg_pr = optimizer_epsf.conjugate_gradient_pr(y0)\n",
    "end = time.time()\n",
    "print \"minimum: {}\\niteration: {}\\ntime: {}s\".format(\n",
    "   result_cg_pr[1], result_cg_pr[2], end - start)\n",
    "print \"---- conjugate gradient-pr end ----\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quasi-Newton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- quasi_newton_bfgs start ----\n",
      "minimum: 0.0740397334593\n",
      "iteration: 35\n",
      "time: 3.4599738121s\n",
      "---- quasi_newton_bfgs end ----\n",
      "\n",
      "---- quasi_newton_dfp start ----\n",
      "minimum: 0.0854359840647\n",
      "iteration: 72\n",
      "time: 4.78077602386s\n",
      "---- quasi_newton_dfp end ----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BFGS\n",
    "print \"---- quasi_newton_bfgs start ----\"\n",
    "start = time.time()\n",
    "result_qn_bfgs = optimizer_epsf.quasi_newton_bfgs(y0)\n",
    "end = time.time()\n",
    "print \"minimum: {}\\niteration: {}\\ntime: {}s\".format(\n",
    "   result_qn_bfgs[1], result_qn_bfgs[2], end - start)\n",
    "print \"---- quasi_newton_bfgs end ----\\n\"\n",
    "\n",
    "# DFP\n",
    "print \"---- quasi_newton_dfp start ----\"\n",
    "start = time.time()\n",
    "result_qn_dfp = optimizer_epsf.quasi_newton_dfp(y0)\n",
    "end = time.time()\n",
    "print \"minimum: {}\\niteration: {}\\ntime: {}s\".format(\n",
    "   result_qn_dfp[1], result_qn_dfp[2], end - start)\n",
    "print \"---- quasi_newton_dfp end ----\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quasi-Newton methods work well with function, converging to minimizer much faster than the above one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limited Memory BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Limited Memory BFGS start ----\n",
      "minimum: 0.0717799128993\n",
      "iteration: 26\n",
      "time: 0.975878953934s\n",
      "---- Limited Memory BFGS end ----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"---- Limited Memory BFGS start ----\"\n",
    "start = time.time()\n",
    "result_qn_l_bfgs = optimizer_epsf.quasi_newton_l_bfgs(y0)\n",
    "end = time.time()\n",
    "print \"minimum: {}\\niteration: {}\\ntime: {}s\".format(\n",
    "   result_qn_l_bfgs[1], result_qn_l_bfgs[2], end - start)\n",
    "print \"---- Limited Memory BFGS end ----\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary ($n=1600$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Method               | Iteration     | Time(s)       | \n",
    "| -------------        |:-------------:| -----:        |\n",
    "| Steepest Descent     | 2175          | 3.0121819973  |\n",
    "| C-G-FR               | 71            | 0.1103601455  |\n",
    "| C-G-PR               | 20            | 0.0370700359  |\n",
    "| BFGS                 | 35            | 3.4599738121  |\n",
    "| DFP                  | 72            | 4.7807760238  |\n",
    "| L-BFGS               | 26            | 0.9758789539  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Dimension         | Memory(BFGS)  | Memory(L-BFGS)| \n",
    "| -------------     |:-------------:| -----:        |\n",
    "| 200               | 1.9 Mb        | $<$ 0.1Mb     |\n",
    "| 400               | 6.7 Mb        | $<$ 0.1Mb     |\n",
    "| 800               | 25  Mb        | $<$ 0.1Mb     |\n",
    "| 1600              | 99.7 Mb       | $<$ 0.1Mb     |\n",
    "| 3200              | 392 Mb        |  0.1Mb        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For constrained problem, I used projected Nesterov's accelarate gradient descent method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from problem2 import f, grad_f, nesterov_1, nesterov_2\n",
    "x0 = np.zeros(1000) # initial point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- unconstrained Nesterov start ----\n",
      "minimum: 5.75535254312e-07\n",
      "iteration: 140\n",
      "time: 0.016471862793s\n",
      "---- unconstrained Nesterov end ----\n",
      "\n",
      "---- Scipy minimize start ----\n",
      "minimum: 1.74715724085e-06\n",
      "iteration: 36\n",
      "time: 9.7752058506s\n",
      "---- Scipy minimize end ----\n",
      "\n",
      "The distance of minimizers found by two algorithms: 0.00141348344192\n"
     ]
    }
   ],
   "source": [
    "# Unconstrained optimization\n",
    "print \"---- unconstrained Nesterov start ----\"\n",
    "start = time.time()\n",
    "result1 = nesterov_1(x0)\n",
    "end = time.time()\n",
    "print \"minimum: {}\\niteration: {}\\ntime: {}s\".format(\n",
    "    result1[1], result1[2], end - start)\n",
    "print \"---- unconstrained Nesterov end ----\\n\"\n",
    "\n",
    "# Scipy's Answer\n",
    "from scipy.optimize import minimize\n",
    "print \"---- Scipy minimize start ----\"\n",
    "start = time.time()\n",
    "res = minimize(f, x0)\n",
    "end = time.time()\n",
    "print \"minimum: {}\\niteration: {}\\ntime: {}s\".format(\n",
    "    res.fun, res.nit, end - start)\n",
    "print \"---- Scipy minimize end ----\\n\"\n",
    "\n",
    "\n",
    "print \"The distance of minimizers found by two algorithms: {}\".format(\n",
    "        np.linalg.norm(result1[0]-res.x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- unconstrained Nesterov start ----\n",
      "minimum: 3.41287079867e-05\n",
      "iteration: 123\n",
      "time: 0.0140008926392s\n",
      "---- unconstrained Nesterov end ----\n",
      "\n",
      "---- Scipy minimize start ----\n",
      "minimum: 0.0153878457611\n",
      "iteration: 7\n",
      "time: 0.357044935226s\n",
      "---- Scipy minimize end ----\n",
      "\n",
      "The distance of minimizers found by two algorithms: 0.00796750237519\n"
     ]
    }
   ],
   "source": [
    "# Constrained optimization\n",
    "print \"---- unconstrained Nesterov start ----\"\n",
    "start = time.time()\n",
    "result2 = nesterov_2(x0)\n",
    "end = time.time()\n",
    "print \"minimum: {}\\niteration: {}\\ntime: {}s\".format(\n",
    "    result2[1], result2[2], end - start)\n",
    "print \"---- unconstrained Nesterov end ----\\n\"\n",
    "\n",
    "bnds = tuple([(0, None)] * 1000)\n",
    "# Scipy's Answer\n",
    "from scipy.optimize import minimize\n",
    "print \"---- Scipy minimize start ----\"\n",
    "start = time.time()\n",
    "res = minimize(f, x0, bounds=bnds)\n",
    "end = time.time()\n",
    "print \"minimum: {}\\niteration: {}\\ntime: {}s\".format(\n",
    "    res.fun, res.nit, end - start)\n",
    "print \"---- Scipy minimize end ----\\n\"\n",
    "\n",
    "print \"The distance of minimizers found by two algorithms: {}\".format(\n",
    "        np.linalg.norm(result2[0]-res.x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code shows that the minimizer obtained from constrained problem actually satisfy the constraints because it is an empty array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2[0][result2[0]<0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
